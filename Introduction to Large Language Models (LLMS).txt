# INTRODUCTION TO LARGE LANGUAGE MODELS (LLMS)
----------------------------------------------

* Chatbots, Language Models and the Birth of AI
-----------------------------------------------
This lesson is an introduction to Large Language Models (LLMs) and text-based Generative Artificial Intelligence (AI). These technologies power many sophisticated text-based AI 
applications we see today, such as ChatGPT, released by the company OpenAI in late 2022. Before we jump into how today’s models function, we’re going to start back in 1966 to understand the history of chatbots, text generation, and the Turing test.

* The ELIZA Effect
------------------
The first digital chatbot, ELIZA, was debuted on the Massachusetts Institute of Technology (MIT) campus in 1966. It was programmed using binary code by computer scientist Joseph 
Weizenbaum. The program generated text by following a few simple rules. These rules were based on a pre-determined “script” that allowed it to assume a specific persona.

For instance, the version of ELIZA that impressed staff and students alike at MIT was known as “DOCTOR”, a kind of barebones psychotherapist who would mirror back statements typed in by 
the user of the program. So if someone typed in “I’m feeling X,” DOCTOR would respond with something like “What is making you feel X?”

ELIZA was very successful in convincing people that they were interacting with a human and not a computer! So much so that there’s even a psychological effect named after the program, the 
ELIZA effect. It refers to the tendency to unconsciously attribute “humanness” to (or anthropomorphize) computer behaviors.

* The Turing Test
-----------------
Chatbots have come a long way since the days of ELIZA. From a computer science perspective, ELIZA and ChatGPT can both be said to pass what is known as the “Turing test”.

In 1950, computer scientist Alan Turing wrote a landmark paper titled “Computing Machinery and Intelligence”, in which he investigated the question:

	What does it mean for machines to think?

He concluded that this question was fundamentally meaningless and said that the better question to ask would be:

	Can a machine successfully imitate human behavior so that it might dupe a human?

To answer this question, he devised a game. The “imitation game”, as he called it, has a human evaluator interacting with a machine and another human through a text-only channel. A 
machine is said to do well at the imitation game if the human evaluator is convinced that they were interacting with another human and not a machine. The imitation game is referred to 
today as the Turing test.

* “AI”: what is it?
-------------------
The Turing test has been a cornerstone in computer science to assess the intelligence of a machine and a lot of the detail lies in the design of the test itself. Six years after Turing’s 
paper was published, a group of scientists put together a proposal for a Summer Research Project on Artificial Intelligence (AI) at Dartmouth, often thought of historically as the moment 
that birthed the field of AI.

Instead of trying to define what AI is or is not, they sought to explain the fundamental assumption behind the pursuit of AI:

	“The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a 
	machine can be made to simulate it.”

Specifically, they were interested in how language might be useful in this pursuit :

	“An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves."

From ELIZA to ChatGPT, there is indeed a marked increase in sophistication, nuance and complexity in the abilities demonstrated by language-based technologies. What has made this possible 
was the move away from “rule-based” chatbots and towards language models that rely on neural networks to capture statistical regularities within language. Let’s dive into how these models 
work!

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Detecting Patterns in Text
----------------------------
The quest to mathematically model language dates back to more than a hundred years. In 1913, mathematician A.A. Markov analyzed letters in a Russian novel and found that there were some 
surprising patterns that could be captured by mathematics. Computer scientist and mathematician Claude Shannon followed up on this analysis a few decades later with a broad question about 
language patterns: Can we predict the next letter given a series of letters?

* Next Letter Predicton
-----------------------
For example, consider the XKCD comic shown to the right where there’s a three letter sequence “eru-”. There are only a few English language words that begin with this combination of 
letters and none of them are very common. The words “erudite”, “erupt” and the less often used “eructate” are a few possible candidates, for instance.

The context of the sentence also narrows down the possibilities for which word can occur next and still make sense.The appearance of “volcano” implies that some variation of the verb “erupt” might be an obvious choice here. The verb tense (in this case, present continuous) gives the final clue that the word “erupt” will appear in the form “erupting”.

* Natural Language Processing (NLP)
-----------------------------------
The explanation above relied on familiarity with the English language. Specifically, we relied on the patterns in language we’ve picked up from reading and speaking to inform our thinking 
as we solved this. Can we train an algorithm to do the same and possibly faster and on many sentences at once? The answer is an emphatic yes and this is the primary goal of the field of 
“Natural Language Processing” (NLP)! To automate this process using a computer, we would need to come up with a way to turn this text into math.

	Many of the initial steps in any NLP task are about standardizing things (i.e. treating language more like numbers, which lend themselves more easily to math) so that computers 
	can do text analysis.

* From Letters to Words to Tokens
---------------------------------
What is the simplest unit of language that text can be broken down to? A letter! A single letter unit is known as a “unigram”. The sentence “Oh my god, the volcano is eru-“ can be broken 
down into a series of unigrams as follows: {o, h, _, m, y, _, g, o, d, _, t, h, e, _, v, o, l, c, a, n, o, _, i, s, _, e, r, u} where the dashes (_) represent blank spaces. We could also 
break it up into two letter units, known as bigrams: where the units would look like: {oh, h_, _m, my, etc,} or three letter units, i.e. trigrams and ultimately n-letter units, known as 
n-grams.

The best choice of the length of the unit depends on the exact problem we’re trying to solve and there are many best practices to follow around this. Formalizing language this way allows 
us to build a language model on a computer to predict the next best unigram, bigram or n-gram.

Is it only letters that we can do this with? Words are a more natural unit of language and we could use them as our smallest unit. We can identify each word unit by the appearance of the 
spaces between them. The Google n-gram viewer uses words as language units to give us statistical insights into the appearance of words in Google Books.

We could also use groups of words or even subwords (like -ing, -ed, etc), often referred to as tokens in the context of language models.

The table to the right shows what this might look like for the sentence in the comic. 

* Instructions
--------------
Search the phrase “artificial intelligence” in Google Books’ n-gram viewer to view the history of the appearance of the phrase in the Google Books data base!

------------------------------------------------------------------------------------------------------------------------------------------------------------------


































































